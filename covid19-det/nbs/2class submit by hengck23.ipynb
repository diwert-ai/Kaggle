{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2class submit by hengck23.ipynb","provenance":[{"file_id":"11qCEgqshsIilyZNW1uZ2O1I2rzvYqR9G","timestamp":1623745009911},{"file_id":"1r6fwPZUfesKm9odwVnB-UyIwzsa0KID6","timestamp":1623701113374},{"file_id":"11i35Q75Lrc9PnrlpHi5mI0aAPI7WAnKw","timestamp":1622307815280}],"collapsed_sections":[],"authorship_tag":"ABX9TyOrp5po7taT0LZed2q5xnD4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"nSexlhNeWHsy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627129724336,"user_tz":-180,"elapsed":295,"user":{"displayName":"Drew Wert","photoUrl":"","userId":"15737305534098303053"}},"outputId":"f7471b60-23b7-4394-ce26-797afab1463e"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Sat Jul 24 12:28:45 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   36C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rMIRX_fD79N8"},"source":["#config consts\n","DATASET =     {'train':'train.tar.gz',\n","               'test':'test.tar.gz',\n","               'train_mask':'train_mask.tar.gz'}\n","\n","METADATA=     {'image_level':'train_image_level.csv',\n","               'study_level':'train_study_level.csv',\n","               'df_meta':'df_meta.csv',\n","               'df_fold_rand830':'df_fold_rand830.csv',\n","               'train_dup':'duplicate.txt',\n","               'sample_sub':'sample_submission.csv'}\n","\n","CFGMODEL_DIR_DICT = {'B3_512':'effb3-full-512-mask-v8/',\n","                     'B4_512':'effb4-full-512-mask/',\n","                     'B5_600':'effb5-600-mask/',\n","                     'B5_640':'effb5-640-mask/',\n","                     'D201_640':'d201-640-mask/',\n","                     'B5_640_2C':'effb5-640-mask-2c/',\n","                     'B7_768_2C':'effb7-768-mask-2c/'}\n","\n","INPUT_DIR ='/content/drive/My\\ Drive/kaggle/covid19-det/input/'\n","\n","OUTPUT_DIR = {'BSL':'/content/drive/My\\ Drive/kaggle/covid19-det/output/',\n","              'NORM':'/content/drive/My Drive/kaggle/covid19-det/output/'}\n","\n","IMPORT_DIR = '/content/drive/My Drive/kaggle/covid19-det/nbs/py/'\n","\n","HENGCK_IM_DIR=IMPORT_DIR+'hengck_code/dummy_01q/'\n","\n","WORK_DIR='/content/'\n","\n","DATASET_DIR_DICT = {'256': INPUT_DIR+'256_jpg/',\n","                    '512': INPUT_DIR+'512_jpg/',\n","                    '600': INPUT_DIR+'600_jpg/',\n","                    '640': INPUT_DIR+'640_jpg/',\n","                    '768': INPUT_DIR+'768_jpg/',}\n","\n","EXPERIMENT='SZ768_2CLASS_5FOLDS'\n","EXPERIMENT_DIR = OUTPUT_DIR['BSL'] + EXPERIMENT+'/'\n","CFGMODEL_DIR = CFGMODEL_DIR_DICT['B7_768_2C']\n","MASK_SIZE=(40,40)\n","DATASET_DIR = DATASET_DIR_DICT['768']\n","METADATA_DIR = INPUT_DIR+'metadata/'\n","FOLDS_SET=[0,1,2,3,4]\n","ENSEMBLE_CSV='/none_768_5f.csv'\n","INITIAL_CHECKPOINTS=['best_model.pth' for i in range(5)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lgTs_sntmBhY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627129738529,"user_tz":-180,"elapsed":14196,"user":{"displayName":"Drew Wert","photoUrl":"","userId":"15737305534098303053"}},"outputId":"b3417942-ecd1-4dd7-bec7-ea7890e621df"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xYx0Py_qWSqK"},"source":["if 0:\n","  !cp -r {OUTPUT_DIR['BSL']+'mpack'} '/content'\n","  !tar zcvf mpack.tar.gz '/content/mpack/'\n","  !cp /content/mpack.tar.gz {OUTPUT_DIR['BSL']}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ejcvBA62Ppy7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627129751131,"user_tz":-180,"elapsed":12606,"user":{"displayName":"Drew Wert","photoUrl":"","userId":"15737305534098303053"}},"outputId":"d0eed55d-3986-4681-b1cc-cfc4276cae6a"},"source":["def copy_dataset(ds_dict, ds_dir, work_dir):\n","  for record in ds_dict:\n","    print('copy', ds_dir+ds_dict[record], ' to', work_dir)\n","    !cp {ds_dir+ds_dict[record]} {work_dir}\n","    print('mkdir',work_dir+record)\n","    !mkdir {work_dir+record}\n","    print ('tar -xzf',work_dir+ds_dict[record],'-C',work_dir+record)\n","    !tar -xzf  {work_dir+ds_dict[record]} -C {work_dir+record}\n","    print ('rm ',work_dir+ds_dict[record])\n","    !rm {work_dir+ds_dict[record]}\n","def copy_metadata(md_dict,md_dir,work_dir):\n","  for record in md_dict:\n","    print('copy ', md_dir+md_dict[record],' to ',work_dir)\n","    !cp {md_dir+md_dict[record]} {work_dir}\n","\n","copy_dataset(DATASET,DATASET_DIR, WORK_DIR)\n","copy_metadata(METADATA,METADATA_DIR,WORK_DIR)\n","!ls /content/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["copy /content/drive/My\\ Drive/kaggle/covid19-det/input/768_jpg/train.tar.gz  to /content/\n","mkdir /content/train\n","tar -xzf /content/train.tar.gz -C /content/train\n","rm  /content/train.tar.gz\n","copy /content/drive/My\\ Drive/kaggle/covid19-det/input/768_jpg/test.tar.gz  to /content/\n","mkdir /content/test\n","tar -xzf /content/test.tar.gz -C /content/test\n","rm  /content/test.tar.gz\n","copy /content/drive/My\\ Drive/kaggle/covid19-det/input/768_jpg/train_mask.tar.gz  to /content/\n","mkdir /content/train_mask\n","tar -xzf /content/train_mask.tar.gz -C /content/train_mask\n","rm  /content/train_mask.tar.gz\n","copy  /content/drive/My\\ Drive/kaggle/covid19-det/input/metadata/train_image_level.csv  to  /content/\n","copy  /content/drive/My\\ Drive/kaggle/covid19-det/input/metadata/train_study_level.csv  to  /content/\n","copy  /content/drive/My\\ Drive/kaggle/covid19-det/input/metadata/df_meta.csv  to  /content/\n","copy  /content/drive/My\\ Drive/kaggle/covid19-det/input/metadata/df_fold_rand830.csv  to  /content/\n","copy  /content/drive/My\\ Drive/kaggle/covid19-det/input/metadata/duplicate.txt  to  /content/\n","copy  /content/drive/My\\ Drive/kaggle/covid19-det/input/metadata/sample_submission.csv  to  /content/\n","df_fold_rand830.csv  sample_data\t    train_image_level.csv\n","df_meta.csv\t     sample_submission.csv  train_mask\n","drive\t\t     test\t\t    train_study_level.csv\n","duplicate.txt\t     train\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"00xbF6PUOJqd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627129769072,"user_tz":-180,"elapsed":17959,"user":{"displayName":"Drew Wert","photoUrl":"","userId":"15737305534098303053"}},"outputId":"21526980-4f64-4cda-bb84-7ec2b63ddc57"},"source":["import sys\n","sys.path.append(HENGCK_IM_DIR)\n","sys.path.append(HENGCK_IM_DIR+CFGMODEL_DIR)\n","\n","!pip install pydicom\n","!pip install madgrad\n","!pip install timm\n","\n","import os\n","os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n","from siim import *\n","\n","\n","from model import *\n","from dataset import *\n","from common import *"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting pydicom\n","  Downloading pydicom-2.1.2-py3-none-any.whl (1.9 MB)\n","\u001b[K     |████████████████████████████████| 1.9 MB 5.1 MB/s \n","\u001b[?25hInstalling collected packages: pydicom\n","Successfully installed pydicom-2.1.2\n","Collecting madgrad\n","  Downloading madgrad-1.1-py3-none-any.whl (7.4 kB)\n","Installing collected packages: madgrad\n","Successfully installed madgrad-1.1\n","Collecting timm\n","  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n","\u001b[K     |████████████████████████████████| 376 kB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.9.0+cu102)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.10.0+cu102)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.7.4.3)\n","Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.19.5)\n","Installing collected packages: timm\n","Successfully installed timm-0.4.12\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Qzt-bjlYQoZJ"},"source":["# start here ! ###################################################################################\n","def probability_to_df_study(df_valid, probability):\n","    df_study = pd.DataFrame()\n","    df_study.loc[:,'id'] = df_valid.study + '_study'\n","    for i in range(num_study_label):\n","        df_study.loc[:,study_name_to_predict_string[study_label_to_name[i]]]=probability[:,i]\n","\n","    df_study = df_study.groupby('id', as_index=False).mean()\n","    df_study.loc[:, 'PredictionString'] = \\\n","           'negative '      + df_study.negative.apply(lambda x: '%0.6f'%x)      + ' 0 0 1 1' \\\n","        + ' typical '       + df_study.typical.apply(lambda x: '%0.6f'%x)       + ' 0 0 1 1' \\\n","        + ' indeterminate ' + df_study.indeterminate.apply(lambda x: '%0.6f'%x) + ' 0 0 1 1' \\\n","        + ' atypical '      + df_study.atypical.apply(lambda x: '%0.6f'%x)      + ' 0 0 1 1'\n","\n","    df_study = df_study[['id','PredictionString']]\n","    return df_study\n","\n","def probability_to_df_none_image(df_valid, probability):\n","  df_none_image = pd.DataFrame()\n","  df_none_image.loc[:,'id']=df_valid.image+'_image'\n","  df_none_image['none']=probability\n","\n","  return df_none_image\n","\n","def probability_to_df_image(df_valid, probability, box):\n","    df_image = pd.DataFrame({'id':[],'PredictionString':[]})\n","    return df_image\n","\n","def do_predict(net, valid_loader, tta=['flip','scale']): #flip\n","\n","    valid_probability = []\n","    valid_num = 0\n","\n","    start_timer = timer()\n","    for t, batch in enumerate(valid_loader):\n","        batch_size = len(batch['index'])\n","        image  = batch['image'].cuda()\n","        label =  batch['none']\n","\n","        #<todo> TTA\n","        net.eval()\n","        with torch.no_grad():\n","            probability = []\n","            logit, mask = net(image)\n","            probability.append(torch.sigmoid(torch.reshape(logit,(-1,))))\n","            \n","            if 'flip' in tta:\n","                logit, mask = net(torch.flip(image,dims=(3,)))\n","                probability.append(torch.sigmoid(torch.reshape(logit,(-1,))))\n","\n","            if 'scale' in tta:\n","                # size=None, scale_factor=None, mode='nearest', align_corners=None, recompute_scale_factor=None):\n","                logit, mask = net(F.interpolate(image, scale_factor=1.33, mode='bilinear', align_corners=False))\n","                probability.append(torch.sigmoid(torch.reshape(logit,(-1,))))\n","\n","            #--------------\n","            probability = torch.stack(probability,0).mean(0)\n","            #print(\"\\nprob: \",probability.shape,probability)\n","\n","        valid_num += batch_size\n","        valid_probability.append(probability.data.cpu().numpy())\n","        print('\\r %8d / %d  %s' % (valid_num, len(valid_loader.dataset), time_to_str(timer() - start_timer, 'sec')),\n","              end='', flush=True)\n","\n","    assert(valid_num == len(valid_loader.dataset))\n","    print('')\n","\n","    probability = np.concatenate(valid_probability)\n","    return probability"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fLj3jzqtQtCj"},"source":["def make_none():\n","    for fold in FOLDS_SET:\n","        out_dir = OUTPUT_DIR['NORM']+EXPERIMENT+'/fold%d'%fold\n","        initial_checkpoint = out_dir + '/checkpoint/'+INITIAL_CHECKPOINTS[fold] # None #\n","\n","        if 1:\n","\n","            ## setup  ----------------------------------------\n","            #mode = 'local'\n","            mode = 'remote'\n","\n","            none_image_dir = out_dir + '/valid/%s'%(mode)\n","            os.makedirs(none_image_dir, exist_ok=True)\n","\n","            log = Logger()\n","            log.open(out_dir + '/log.none_image.txt', mode='a')\n","            log.write('\\n--- [START %s] %s\\n\\n' % (IDENTIFIER, '-' * 64))\n","            log.write('\\t%s\\n' % COMMON_STRING)\n","            log.write('\\n')\n","\n","            #\n","            ## dataset ------------------------------------\n","\n","            if 'remote' in mode: #1263\n","                df_valid = make_fold('test')\n","\n","            if 'local' in mode: #1276 #1256\n","                df_train, df_valid = make_fold('train-%d' % fold)\n","\n","\n","            valid_dataset = SiimDataset(df_valid)\n","            valid_loader  = DataLoader(\n","                valid_dataset,\n","                sampler = SequentialSampler(valid_dataset),\n","                batch_size  = 16,#128, #\n","                drop_last   = False,\n","                num_workers = 2,\n","                pin_memory  = True,\n","                collate_fn  = null_collate,\n","            )\n","            log.write('mode : %s\\n'%(mode))\n","            log.write('valid_dataset : \\n%s\\n'%(valid_dataset))\n","\n","            ## net ----------------------------------------\n","            if 1:\n","                net = Net().cuda()\n","                net.load_state_dict(torch.load(initial_checkpoint)['state_dict'], strict=True)\n","\n","                #---\n","                start_timer = timer()\n","                probability = do_predict(net, valid_loader)\n","                log.write('time %s \\n' % time_to_str(timer() - start_timer, 'min'))\n","                log.write('probability %s \\n' % str(probability.shape))\n","\n","                np.save(none_image_dir + '/probability.npy',probability)\n","                df_valid.to_csv(none_image_dir + '/df_valid.csv', index=False)\n","\n","            else:\n","                probability = np.load(none_image_dir + '/probability.npy')\n","\n","            #----\n","            #df_study = probability_to_df_study(df_valid, probability)\n","            #df_image = probability_to_df_image(df_valid, None, None)\n","            df_none_image = probability_to_df_none_image(df_valid,probability)\n","            #df_submit = pd.concat([df_study,df_image])\n","            df_none_image.to_csv(none_image_dir + '/none_image.csv', index=False)\n","\n","            log.write('none_image_dir : %s\\n' % (none_image_dir))\n","            log.write('initial_checkpoint : %s\\n' % (initial_checkpoint))\n","            log.write('df_none_image : %s\\n' % str(df_none_image.shape))\n","            log.write('%s\\n' % str(df_none_image))\n","            log.write('\\n')\n","\n","            if 'local' in mode:\n","                onehot = df_valid[study_name_to_label.keys()].values\n","                truth = onehot.argmax(-1)\n","                predict = probability.argsort(-1)[:,::-1]\n","\n","                loss = np_loss_cross_entropy(probability, truth)\n","                topk = (predict == truth.reshape(-1, 1))\n","                acc  = topk[:, 0]\n","                topk = topk.mean(0).cumsum()\n","                acc  = [acc[truth == i].mean() for i in range(num_study_label)]\n","\n","                # ---\n","                map = np_metric_map_curve_by_class(probability, truth)\n","\n","                # ---\n","                log.write('loss : %f\\n' % (loss))\n","                log.write('topk : %s\\n' % str(topk))\n","                log.write('map(mean) : %f\\n' % map.mean())\n","                log.write('          : %f\\n' % ((4 / 6)* map.mean()))\n","                for i in range(num_study_label):\n","                    l = study_label_to_name[i]\n","                    log.write('%d %30s : %0.5f\\n' % (i,l,map[i]))\n","                log.write('\\n\\n')\n","        #exit(0)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N6gKkFHwQu3H"},"source":["def make_none_ensemble():\n","    out_dir = OUTPUT_DIR['NORM']+EXPERIMENT\n","    log = Logger()\n","    log.open(out_dir + '/log.submit.txt', mode='a')\n","    log.write('\\n--- [START %s] %s\\n\\n' % (IDENTIFIER, '-' * 64))\n","    log.write('\\t%s\\n' % COMMON_STRING)\n","    log.write('\\n')\n","\n","    none_image_dir=[]\n","    for fold in FOLDS_SET:\n","      none_image_dir.append(out_dir+'/fold%d/valid/remote/'%fold)\n","    \n","    probability=0\n","    for d in none_image_dir:\n","        p = np.load(d + '/probability.npy')\n","        probability += p\n","    probability = probability/len(none_image_dir)\n","\n","\n","    #----\n","    df_valid = pd.read_csv(none_image_dir[0] + '/df_valid.csv')\n","\n","    #df_study  = probability_to_df_study(df_valid, probability)\n","    #df_image  = probability_to_df_image(df_valid, None, None)\n","    df_none_image = probability_to_df_none_image(df_valid,probability)\n","    df_none_image.to_csv(out_dir + ENSEMBLE_CSV, index=False)\n","\n","\n","    log.write('none_image_dir : %s\\n' % (none_image_dir))\n","    log.write('df_none_image : %s\\n' % str(df_none_image.shape))\n","    log.write('%s\\n' % str(df_none_image))\n","    log.write('\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YTvnKVkBSUrp"},"source":["make_none()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O5np8LWnJG6l"},"source":["make_none_ensemble()"],"execution_count":null,"outputs":[]}]}