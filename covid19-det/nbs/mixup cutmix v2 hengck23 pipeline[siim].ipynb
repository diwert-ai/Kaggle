{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mixup cutmix v2 hengck23 pipeline[siim].ipynb","provenance":[{"file_id":"1r6fwPZUfesKm9odwVnB-UyIwzsa0KID6","timestamp":1623701113374},{"file_id":"11i35Q75Lrc9PnrlpHi5mI0aAPI7WAnKw","timestamp":1622307815280}],"collapsed_sections":[],"authorship_tag":"ABX9TyPVLnmAUXK7akoovQpBnWi3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2XQH3OaLq1-_","executionInfo":{"status":"ok","timestamp":1625749825698,"user_tz":-180,"elapsed":345,"user":{"displayName":"Drew Wert","photoUrl":"","userId":"15737305534098303053"}},"outputId":"b5f8ed93-57cb-4752-f4ce-66c232604cc5"},"source":["!nvidia-smi"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Thu Jul  8 13:10:25 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   46C    P0    31W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rMIRX_fD79N8","executionInfo":{"status":"ok","timestamp":1625749832850,"user_tz":-180,"elapsed":325,"user":{"displayName":"Drew Wert","photoUrl":"","userId":"15737305534098303053"}}},"source":["#config consts\n","DATASET =     {'train':'train.tar.gz',\n","               'test':'test.tar.gz',\n","               'train_mask':'train_mask.tar.gz'}\n","\n","METADATA=     {'image_level':'train_image_level.csv',\n","               'study_level':'train_study_level.csv',\n","               'df_meta':'df_meta.csv',\n","               'df_fold_rand830':'df_fold_rand830.csv',\n","               'train_dup':'duplicate.txt',\n","               'sample_sub':'sample_submission.csv'}\n","\n","CFGMODEL_DIR_DICT = {'B3_512':'effb3-full-512-mask-v8/',\n","                     'B3_512_PNG': 'effb3-512-png-mask/',\n","                     'B4_512':'effb4-full-512-mask/',\n","                     'B5_600':'effb5-600-mask/',\n","                     'B5_640':'effb5-640-mask/',\n","                     'B5_640_M2': 'effb5-640-mask/',\n","                     'D121_640':'d121-640-mask/',\n","                     'D201_640':'d201-640-mask/'}\n","\n","INPUT_DIR ='/content/drive/My\\ Drive/kaggle/covid19-det/input/'\n","\n","OUTPUT_DIR = {'BSL':'/content/drive/My\\ Drive/kaggle/covid19-det/output/',\n","              'NORM':'/content/drive/My Drive/kaggle/covid19-det/output/'}\n","\n","IMPORT_DIR = '/content/drive/My Drive/kaggle/covid19-det/nbs/py/'\n","\n","HENGCK_IM_DIR=IMPORT_DIR+'hengck_code/dummy_01q/'\n","\n","WORK_DIR='/content/'\n","\n","DATASET_DIR_DICT = {'256': INPUT_DIR+'256_jpg/',\n","                    '512': INPUT_DIR+'512_jpg/',\n","                    '512_PNG': INPUT_DIR+'512_png/',\n","                    '600': INPUT_DIR+'600_jpg/',\n","                    '640': INPUT_DIR+'640_jpg/',\n","                    '640_M2':INPUT_DIR+'640_jpg_m2/'}\n","\n","EXPERIMENT='SZ640_D201_MASK_0FOLD_WL4_MIXUP_CUTMIX'\n","EXPERIMENT_DIR = OUTPUT_DIR['BSL'] + EXPERIMENT+'/'\n","CFGMODEL_DIR = CFGMODEL_DIR_DICT['D201_640']\n","MASK_SIZE=(20,20)\n","L1_FACTOR=4\n","DATASET_DIR = DATASET_DIR_DICT['640']\n","METADATA_DIR = INPUT_DIR+'metadata/'\n","FOLDS_SET=[0]\n","INITIAL_CHECKPOINTS=[None for i in range(5)]"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"lgTs_sntmBhY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625749856711,"user_tz":-180,"elapsed":16829,"user":{"displayName":"Drew Wert","photoUrl":"","userId":"15737305534098303053"}},"outputId":"c6768d41-7a86-4ebb-b870-a86fb688b1d4"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aTWvq4iJPT07","executionInfo":{"status":"ok","timestamp":1625749898148,"user_tz":-180,"elapsed":28159,"user":{"displayName":"Drew Wert","photoUrl":"","userId":"15737305534098303053"}},"outputId":"f545fc77-e018-4ffa-cbc4-f647e6253364"},"source":["!pip install pydicom\n","!pip install madgrad\n","!pip install timm\n","\n","import sys\n","sys.path.append(HENGCK_IM_DIR)\n","sys.path.append(HENGCK_IM_DIR+CFGMODEL_DIR)\n","\n","import os\n","os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n","from common import *\n","\n","from siim import *\n","\n","# from lib.net.lookahead import *\n","# from lib.net.radam import *\n","from madgrad import MADGRAD\n","\n","from model import *\n","from dataset import *"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting pydicom\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/15/df16546bc59bfca390cf072d473fb2c8acd4231636f64356593a63137e55/pydicom-2.1.2-py3-none-any.whl (1.9MB)\n","\u001b[K     |████████████████████████████████| 1.9MB 11.4MB/s \n","\u001b[?25hInstalling collected packages: pydicom\n","Successfully installed pydicom-2.1.2\n","Collecting madgrad\n","  Downloading https://files.pythonhosted.org/packages/65/f0/4584f18202a2fb8903d456bf907b80e7cb54ad8fcba68604084ff41b7cf8/madgrad-1.1-py3-none-any.whl\n","Installing collected packages: madgrad\n","Successfully installed madgrad-1.1\n","Collecting timm\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/fc/606bc5cf46acac3aa9bd179b3954433c026aaf88ea98d6b19f5d14c336da/timm-0.4.12-py3-none-any.whl (376kB)\n","\u001b[K     |████████████████████████████████| 378kB 11.8MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.9.0+cu102)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.10.0+cu102)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.7.4.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.19.5)\n","Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n","Installing collected packages: timm\n","Successfully installed timm-0.4.12\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5xthKKVNJ7gp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625749941005,"user_tz":-180,"elapsed":15062,"user":{"displayName":"Drew Wert","photoUrl":"","userId":"15737305534098303053"}},"outputId":"9c6e0679-f885-4714-91f1-f2a9c4d15cb5"},"source":["def copy_dataset(ds_dict, ds_dir, work_dir):\n","  for record in ds_dict:\n","    print('copy', ds_dir+ds_dict[record], ' to', work_dir)\n","    !cp {ds_dir+ds_dict[record]} {work_dir}\n","    print('mkdir',work_dir+record)\n","    !mkdir {work_dir+record}\n","    print ('tar -xzf',work_dir+ds_dict[record],'-C',work_dir+record)\n","    !tar -xzf  {work_dir+ds_dict[record]} -C {work_dir+record}\n","    print ('rm ',work_dir+ds_dict[record])\n","    !rm {work_dir+ds_dict[record]}\n","def copy_metadata(md_dict,md_dir,work_dir):\n","  for record in md_dict:\n","    print('copy ', md_dir+md_dict[record],' to ',work_dir)\n","    !cp {md_dir+md_dict[record]} {work_dir}\n","copy_dataset(DATASET,DATASET_DIR, WORK_DIR)\n","copy_metadata(METADATA,METADATA_DIR,WORK_DIR)\n","!ls /content/"],"execution_count":5,"outputs":[{"output_type":"stream","text":["copy /content/drive/My\\ Drive/kaggle/covid19-det/input/640_jpg/train.tar.gz  to /content/\n","mkdir /content/train\n","tar -xzf /content/train.tar.gz -C /content/train\n","rm  /content/train.tar.gz\n","copy /content/drive/My\\ Drive/kaggle/covid19-det/input/640_jpg/test.tar.gz  to /content/\n","mkdir /content/test\n","tar -xzf /content/test.tar.gz -C /content/test\n","rm  /content/test.tar.gz\n","copy /content/drive/My\\ Drive/kaggle/covid19-det/input/640_jpg/train_mask.tar.gz  to /content/\n","mkdir /content/train_mask\n","tar -xzf /content/train_mask.tar.gz -C /content/train_mask\n","rm  /content/train_mask.tar.gz\n","copy  /content/drive/My\\ Drive/kaggle/covid19-det/input/metadata/train_image_level.csv  to  /content/\n","copy  /content/drive/My\\ Drive/kaggle/covid19-det/input/metadata/train_study_level.csv  to  /content/\n","copy  /content/drive/My\\ Drive/kaggle/covid19-det/input/metadata/df_meta.csv  to  /content/\n","copy  /content/drive/My\\ Drive/kaggle/covid19-det/input/metadata/df_fold_rand830.csv  to  /content/\n","copy  /content/drive/My\\ Drive/kaggle/covid19-det/input/metadata/duplicate.txt  to  /content/\n","copy  /content/drive/My\\ Drive/kaggle/covid19-det/input/metadata/sample_submission.csv  to  /content/\n","df_fold_rand830.csv  sample_data\t    train_image_level.csv\n","df_meta.csv\t     sample_submission.csv  train_mask\n","drive\t\t     test\t\t    train_study_level.csv\n","duplicate.txt\t     train\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EI6ls8CoREVD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625749948320,"user_tz":-180,"elapsed":382,"user":{"displayName":"Drew Wert","photoUrl":"","userId":"15737305534098303053"}},"outputId":"8c915259-e1fb-49a7-fc2f-0ef27babb971"},"source":["!mkdir {EXPERIMENT_DIR}"],"execution_count":6,"outputs":[{"output_type":"stream","text":["mkdir: cannot create directory ‘/content/drive/My Drive/kaggle/covid19-det/output/SZ640_D201_MASK_0FOLD_WL4_MIXUP_CUTMIX/’: File exists\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Qzt-bjlYQoZJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625749961192,"user_tz":-180,"elapsed":10453,"user":{"displayName":"Drew Wert","photoUrl":"","userId":"15737305534098303053"}},"outputId":"300798d3-8a94-4372-f2ab-50d3ac2c22bf"},"source":["#----------------\n","import torch.cuda.amp as amp\n","\n","class AmpNet(Net):\n","    @torch.cuda.amp.autocast()\n","    def forward(self,*args):\n","        return super(AmpNet, self).forward(*args)\n","\n","is_mixed_precision = True  #True #False\n","run_check_net()"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/densenet201-c1103571.pth\" to /root/.cache/torch/hub/checkpoints/densenet201-c1103571.pth\n"],"name":"stderr"},{"output_type":"stream","text":["image:  torch.Size([2, 3, 640, 640])\n","logit:  torch.Size([2, 4])\n","mask:  torch.Size([2, 1, 20, 20])\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"fLj3jzqtQtCj","executionInfo":{"status":"ok","timestamp":1625749968539,"user_tz":-180,"elapsed":325,"user":{"displayName":"Drew Wert","photoUrl":"","userId":"15737305534098303053"}}},"source":["#----------------\n","def train_augment(r):\n","    image = r['image']\n","    mask = r['mask']\n","    # if image[:2].shape != (image_size, image_size):\n","    #     image = cv2.resize(image, dsize=(image_size, image_size), interpolation=cv2.INTER_AREA)\n","\n","    if 1:\n","        for fn in np.random.choice([\n","            lambda image, mask : do_random_scale(image, mask, mag=0.20),\n","            lambda image, mask : do_random_stretch_y(image, mask, mag=0.20),\n","            lambda image, mask : do_random_stretch_x(image, mask, mag=0.20),\n","            lambda image, mask : do_random_shift(image, mask, mag=int(0.20*image_size)),\n","            lambda image, mask : (image, mask)\n","        ],1):\n","            image, mask = fn(image, mask)\n","\n","        for fn in np.random.choice([\n","            lambda image, mask : do_random_rotate(image, mask, mag=15),\n","            lambda image, mask : do_random_hflip(image, mask),\n","            lambda image, mask : (image, mask)\n","        ],1):\n","            image, mask = fn(image, mask)\n","\n","        # ------------------------\n","        for fn in np.random.choice([\n","            lambda image : do_random_intensity_shift_contast(image, mag=[0.5,0.5]),\n","            lambda image : do_random_noise(image, mag=0.05),\n","            lambda image : do_random_guassian_blur(image),\n","            lambda image : do_random_blurout(image, size=0.25, num_cut=2),\n","            #lambda image : do_random_clahe(image),\n","            #lambda image : do_histogram_norm(image),\n","            lambda image : image,\n","        ],1):\n","            image = fn(image)\n","\n","    r['image'] = image\n","    r['mask'] = mask\n","    return r\n","    \n","def do_valid(net, valid_loader):\n","\n","    valid_probability = []\n","    valid_truth = []\n","    valid_num = 0\n","\n","    net.eval()\n","    start_timer = timer()\n","    for t, batch in enumerate(valid_loader):\n","        batch_size = len(batch['index'])\n","        image = batch['image'].cuda()\n","        onehot = batch['onehot']\n","        label = onehot.argmax(-1)\n","\n","        with torch.no_grad():\n","            #with amp.autocast():\n","                logit, mask = data_parallel(net,image)\n","                probability = F.softmax(logit,-1)\n","\n","        valid_num += batch_size\n","        valid_probability.append(probability.data.cpu().numpy())\n","        valid_truth.append(label.data.cpu().numpy())\n","        print('\\r %8d / %d  %s'%(valid_num, len(valid_loader.dataset),time_to_str(timer() - start_timer,'sec')),end='',flush=True)\n","\n","    assert(valid_num == len(valid_loader.dataset))\n","    #print('')\n","    #----------------------\n","    truth = np.concatenate(valid_truth)\n","    probability = np.concatenate(valid_probability)\n","    predict = probability.argsort(-1)[:,::-1]\n","\n","    loss = np_loss_cross_entropy(probability,truth)\n","    topk = (predict==truth.reshape(-1,1))\n","    acc  = topk[:, 0]\n","    topk = topk.mean(0).cumsum()\n","    acc = [acc[truth==i].mean() for i in range(num_study_label)]\n","\n","    #---\n","    map  = np_metric_map_curve_by_class(probability, truth)*(4/6)\n","\n","    return [loss, map.mean(), topk[0], topk[1]]"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"OpXp3eXIuESC","executionInfo":{"status":"ok","timestamp":1625760707978,"user_tz":-180,"elapsed":595,"user":{"displayName":"Drew Wert","photoUrl":"","userId":"15737305534098303053"}}},"source":["INITIAL_CHECKPOINTS=['best_model.pth' for i in range(5)]"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IejqWhJl4AaO","executionInfo":{"status":"ok","timestamp":1625760710859,"user_tz":-180,"elapsed":430,"user":{"displayName":"Drew Wert","photoUrl":"","userId":"15737305534098303053"}},"outputId":"cafcecb5-3bcd-439a-f464-aa39fbe07e35"},"source":["INITIAL_CHECKPOINTS"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['best_model.pth',\n"," 'best_model.pth',\n"," 'best_model.pth',\n"," 'best_model.pth',\n"," 'best_model.pth']"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"r9UvULISzAfR","executionInfo":{"status":"ok","timestamp":1625749978446,"user_tz":-180,"elapsed":510,"user":{"displayName":"Drew Wert","photoUrl":"","userId":"15737305534098303053"}}},"source":["def rand_bboxes(size, gamma):\n","    W = size[0]\n","    H = size[1]\n","    GS = gamma.shape[0]\n","    cut_rat = np.sqrt(1. - gamma)\n","    cut_w = np.int_(W * cut_rat)\n","    cut_h = np.int_(H * cut_rat)\n","\n","    # uniform\n","    cx = np.random.randint(W,size=GS)\n","    cy = np.random.randint(H,size=GS)\n","\n","\n","    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n","    bby1 = np.clip(cy - cut_h // 2, 0, H)\n","    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n","    bby2 = np.clip(cy + cut_h // 2, 0, H)\n"," \n","    return bbx1, bby1, bbx2, bby2"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"oZtrGvsWkIRi","executionInfo":{"status":"ok","timestamp":1625750018954,"user_tz":-180,"elapsed":503,"user":{"displayName":"Drew Wert","photoUrl":"","userId":"15737305534098303053"}}},"source":["def do_mixup(batch1,batch2,alpha=0.4):\n","  bs = len(batch1['index'])\n","  gamma = np.random.beta(alpha, alpha, bs)\n","  gamma = torch.FloatTensor(np.max(np.stack((gamma,1-gamma)),axis=0))\n","  gamma_4 = gamma[:,None,None,None].cuda()\n","  gamma_2 = gamma[:,None].cuda()\n","  mix_image = gamma_4*batch1['image'].cuda()+(1-gamma_4)*batch2['image'].cuda()\n","  mix_mask = gamma_4*batch1['mask'].cuda()+(1-gamma_4)*batch2['mask'].cuda()\n","  mix_onehot = gamma_2*batch1['onehot'].cuda()+(1-gamma_2)*batch2['onehot'].cuda()\n"," \n","  mix_batch={'image':mix_image,\n","             'mask': mix_mask,\n","             'onehot':mix_onehot,\n","             'gamma': gamma}\n","\n","  return mix_batch"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"37_V5sFjx9wq","executionInfo":{"status":"ok","timestamp":1625750022086,"user_tz":-180,"elapsed":2,"user":{"displayName":"Drew Wert","photoUrl":"","userId":"15737305534098303053"}}},"source":["def do_cutmix(batch1, batch2,alpha=0.4):\n","    bs = len(batch1['index'])\n","    gamma = np.random.beta(alpha, alpha,bs)\n","    gamma = torch.FloatTensor(np.max(np.stack((gamma,1-gamma)),axis=0))\n","    bbx1, bby1, bbx2, bby2 = rand_bboxes((image_size,image_size), gamma.numpy())\n","\n","    cutmix_image = batch1['image'].detach().clone().cuda()\n","    cutmix_mask = batch1['mask'].detach().clone().cuda()\n","    for i in range(bs):\n","      cutmix_image[i, :, bbx1[i]:bbx2[i], bby1[i]:bby2[i]] = batch2['image'][i, :, bbx1[i]:bbx2[i], bby1[i]:bby2[i]]\n","      cutmix_mask[i, :, bbx1[i]:bbx2[i], bby1[i]:bby2[i]] = batch2['mask'][i, :, bbx1[i]:bbx2[i], bby1[i]:bby2[i]]\n","\n","    # adjust gamma to exactly match pixel ratio\n","    gamma = torch.FloatTensor(1 - ((bbx2 - bbx1) * (bby2 - bby1) / (image_size*image_size)))\n","    gamma_2 = gamma[:,None].cuda()\n","    \n","    cutmix_onehot = gamma_2*batch1['onehot'].cuda()+(1-gamma_2)*batch2['onehot'].cuda()\n","\n","    cutmix_batch = {'image': cutmix_image,\n","                    'mask': cutmix_mask,\n","                    'onehot': cutmix_onehot,\n","                    'gamma': gamma}\n","\n","    \n","    return cutmix_batch"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"y50BApv-AX7l","executionInfo":{"status":"ok","timestamp":1625750026383,"user_tz":-180,"elapsed":324,"user":{"displayName":"Drew Wert","photoUrl":"","userId":"15737305534098303053"}}},"source":["def draw_batch(imbatch):\n","  bs = imbatch.shape[0]\n","  fig, axs = plt.subplots(1, bs, figsize=(30, 30))\n","  for i in range(bs):\n","    axs[i].imshow(imbatch[i,0,:,:],cmap='gray')\n","  plt.show()\n"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1hVU2QflLHa4OHl39y6jwJf_McbGf8ibq"},"id":"CHHEvx5M6idI","executionInfo":{"status":"ok","timestamp":1625750198865,"user_tz":-180,"elapsed":9070,"user":{"displayName":"Drew Wert","photoUrl":"","userId":"15737305534098303053"}},"outputId":"62452913-6514-440f-96a4-841b10e865b6"},"source":["#debug\n","if 0:\n","  df_train_deb, _ = make_fold('train-0')\n","  train_dataset_deb = SiimDataset(df_train_deb, train_augment)\n","  train_loader_deb = DataLoader(\n","            train_dataset_deb,\n","            sampler = RandomSampler(train_dataset_deb),\n","            batch_size = 4,\n","            drop_last   = True,\n","            num_workers = 2,\n","            pin_memory  = True,\n","            worker_init_fn=lambda id: np.random.seed(torch.initial_seed() // 2 ** 32 + id),\n","            collate_fn  = null_collate,\n","        )\n","  it = iter(train_loader_deb)\n","  batch1=next(it)\n","  batch2=next(it)\n","  cutmix_batch = do_cutmix(batch1,batch2)\n","  mixup_batch = do_mixup(batch1,batch2)\n","  print(np.sum(cutmix_batch['onehot'].cpu().numpy(),axis=1))\n","  print(np.sum(mixup_batch['onehot'].cpu().numpy(),axis=1))\n","  for key in ['image','mask']:\n","    draw_batch(batch1[key])\n","    draw_batch(batch2[key])\n","    draw_batch(mixup_batch[key].cpu())\n","    draw_batch(cutmix_batch[key].cpu())"],"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"SonYuSirvgAc","executionInfo":{"status":"ok","timestamp":1625781977894,"user_tz":-180,"elapsed":935,"user":{"displayName":"Drew Wert","photoUrl":"","userId":"15737305534098303053"}}},"source":["def run_train_mixup_cutmix():\n","    for fold in FOLDS_SET:\n","        out_dir = OUTPUT_DIR['NORM']+EXPERIMENT+'/fold%d-fine5'%fold\n","        \n","        initial_checkpoint=None\n","        if INITIAL_CHECKPOINTS[fold] is not None: \n","          initial_checkpoint = OUTPUT_DIR['NORM']+EXPERIMENT+'/fold%d-fine4/checkpoint/'%fold+INITIAL_CHECKPOINTS[fold]\n","\n","        best_map = 0.3\n","        start_lr   = 0.00001#1\n","        batch_size = 8 #14 #22\n","\n","        num_iteration = 12000\n","        iter_log    = 200\n","        iter_valid  = 200\n","        iter_save   = list(range(0, num_iteration+1, 200))\n","        a_iter_save = []\n","\n","        ## setup  ----------------------------------------\n","        for f in ['checkpoint', 'train', 'valid', 'backup']: os.makedirs(out_dir + '/' + f, exist_ok=True)\n","        # backup_project_as_zip(PROJECT_PATH, out_dir +'/backup/code.train.%s.zip'%IDENTIFIER)\n","\n","        log = Logger()\n","        log.open(out_dir + '/log.train.txt', mode='a')\n","        log.write('\\n--- [START %s] %s\\n\\n' % (IDENTIFIER, '-' * 64))\n","        log.write('\\t%s\\n' % COMMON_STRING)\n","        log.write('\\texpirement = %s\\n' % EXPERIMENT)\n","        log.write('\\tout_dir  = %s\\n' % out_dir)\n","        log.write('\\n')\n","\n","        ## dataset ------------------------------------\n","        df_train, df_valid = make_fold('train-%d'%fold)\n","        train_dataset = SiimDataset(df_train, train_augment)\n","        valid_dataset = SiimDataset(df_valid, )\n","\n","        train_loader1 = DataLoader(\n","            train_dataset,\n","            sampler = RandomSampler(train_dataset),\n","            batch_size = batch_size,\n","            drop_last   = True,\n","            num_workers = 2,\n","            pin_memory  = True,\n","            worker_init_fn=lambda id: np.random.seed(torch.initial_seed() // 2 ** 32 + id),\n","            collate_fn  = null_collate,\n","        )\n","        train_loader2 = DataLoader(\n","            train_dataset,\n","            sampler = RandomSampler(train_dataset),\n","            batch_size = batch_size,\n","            drop_last   = True,\n","            num_workers = 2,\n","            pin_memory  = True,\n","            worker_init_fn=lambda id: np.random.seed(torch.initial_seed() // 2 ** 32 + id),\n","            collate_fn  = null_collate,\n","        )\n","        valid_loader  = DataLoader(\n","            valid_dataset,\n","            sampler = SequentialSampler(valid_dataset),\n","            batch_size  = 16,\n","            drop_last   = False,\n","            num_workers = 2,\n","            pin_memory  = True,\n","            collate_fn  = null_collate,\n","        )\n","\n","        log.write('train_dataset : \\n%s\\n'%(train_dataset))\n","        log.write('valid_dataset : \\n%s\\n'%(valid_dataset))\n","        log.write('\\n')\n","\n","\n","        ## net ----------------------------------------\n","        log.write('** net setting **\\n')\n","        if is_mixed_precision:\n","            scaler = amp.GradScaler()\n","            net = AmpNet().cuda()\n","        else:\n","            net = Net().cuda()\n","\n","\n","        if initial_checkpoint is not None:\n","            f = torch.load(initial_checkpoint, map_location=lambda storage, loc: storage)\n","            start_iteration = f['iteration']\n","            num_iteration = start_iteration + 12000\n","            start_epoch = f['epoch']\n","            state_dict  = f['state_dict']\n","            net.load_state_dict(state_dict,strict=True)  #True\n","        else:\n","            start_iteration = 0\n","            start_epoch = 0\n","\n","\n","        log.write('net=%s\\n'%(type(net)))\n","        log.write('\\tinitial_checkpoint = %s\\n' % initial_checkpoint)\n","        log.write('\\n')\n","\n","        # -----------------------------------------------\n","        if 0: ##freeze\n","            for p in net.block0.backbone.parameters(): p.requires_grad = False\n","\n","\n","        #optimizer = Lookahead(RAdam(filter(lambda p: p.requires_grad, net.parameters()),lr=start_lr), alpha=0.5, k=5)\n","        #optimizer = RAdam(filter(lambda p: p.requires_grad, net.parameters()),lr=start_lr)\n","        optimizer = MADGRAD( filter(lambda p: p.requires_grad, net.parameters()), lr=start_lr, momentum= 0.9, weight_decay= 0, eps= 1e-06)\n","\n","\n","        # num_iteration = 8000\n","        # iter_log    = 100\n","        # iter_valid  = 100\n","        # iter_save   = list(range(0, num_iteration, 100))#1*1000\n","\n","        log.write('optimizer\\n  %s\\n'%(optimizer))\n","        log.write('\\n')\n","\n","\n","        ## start training here! ##############################################\n","        log.write('** start training here! **\\n')\n","        log.write('   fold = %d\\n'%(fold))\n","        log.write('   is_mixed_precision = %s \\n'%str(is_mixed_precision))\n","        log.write('   batch_size = %d\\n'%(batch_size))\n","        log.write('   experiment = %s\\n' % str(EXPERIMENT.split('/')[-2:]))\n","        log.write('                      |-----------VALID-------------|-----TRAIN/BATCH -----|------TIME-----\\n')\n","        log.write('rate     iter   epoch | loss    map   topk0  topk1  | loss0  loss1  loss2  |               \\n')\n","        log.write('-------------------------------------------------------------------------------------------\\n')\n","                  #0.00000  0.00*  0.00  | 0.000  0.000  0.000  0.000  | 0.000  0.000  0.000  |  0 hr 00 min\n","\n","        def message(mode='print'):\n","            if mode==('print'):\n","                asterisk = ' '\n","                loss = batch_loss\n","            if mode==('log'):\n","                asterisk = '*' if iteration in a_iter_save else ' '\n","                loss = train_loss\n","\n","            text = \\\n","                '%0.5f  %5.3f%s %4.2f  | '%(rate, iteration/10000, asterisk, epoch,) +\\\n","                '%4.3f  %4.3f  %4.3f  %4.3f  | '%(*valid_loss,) +\\\n","                '%4.3f  %4.3f  %4.3f  | '%(*loss,) +\\\n","                '%s' % (time_to_str(timer() - start_timer,'min'))\n","\n","            return text\n","\n","        #----\n","        valid_loss = np.zeros(4,np.float32)\n","        train_loss = np.zeros(3,np.float32)\n","        batch_loss = np.zeros_like(train_loss)\n","        sum_train_loss = np.zeros_like(train_loss)\n","        sum_train = 0\n","        loss0 = torch.FloatTensor([0]).cuda().sum()\n","        loss1 = torch.FloatTensor([0]).cuda().sum()\n","        loss2 = torch.FloatTensor([0]).cuda().sum()\n","\n","\n","        start_timer = timer()\n","        iteration = start_iteration\n","        epoch = start_epoch\n","        rate = 0\n","\n","        while  iteration < num_iteration:\n","            it_loader2 = iter(train_loader2)\n","            for t, batch1 in enumerate(train_loader1):\n","                \n","\n","                if (iteration % iter_valid == 0):\n","                    #if iteration!=start_iteration:\n","                        valid_loss = do_valid(net, valid_loader)  #\n","                        pass\n","\n","\n","                if best_map < valid_loss[1]:\n","                  best_map = valid_loss[1]\n","                  a_iter_save.append(iteration)\n","                  torch.save({\n","                            'state_dict': net.state_dict(),\n","                            'iteration': iteration,\n","                            'epoch': epoch,\n","                        }, out_dir + '/checkpoint/best_model.pth')\n","                  pass\n","\n","                if (iteration % iter_log == 0):\n","                    print('\\r', end='', flush=True)\n","                    log.write(message(mode='log') + '\\n')\n","\n","\n","                # learning rate schduler ------------\n","                rate = get_learning_rate(optimizer)\n","\n","                # mixup cutmix--------------\n","              \n","                with torch.no_grad():\n","                  prob = np.random.rand()\n","                  if prob<0.33:\n","                    batch2 = next(it_loader2)\n","                    mix_batch = do_mixup(batch1,batch2)\n","                  elif prob>=0.33 and prob<0.66:\n","                    batch2 = next(it_loader2)\n","                    mix_batch = do_cutmix(batch1,batch2)\n","                  else:\n","                    mix_batch = {'image' :batch1['image'].cuda(),\n","                                 'mask'  :batch1['mask'].cuda(),\n","                                 'onehot':batch1['onehot'].cuda()}\n","\n","\n","                # one iteration update  -------------\n","                batch_size = len(batch1['index'])\n","                image = mix_batch['image']\n","                truth_mask = mix_batch['mask']\n","                truth_mask = F.interpolate(truth_mask, size=MASK_SIZE, mode='bilinear', align_corners=False)\n","                onehot = mix_batch['onehot']\n","                label = onehot.argmax(-1)\n","\n","                #----\n","                net.train()\n","                optimizer.zero_grad()\n","\n","                if is_mixed_precision:\n","                    with amp.autocast():\n","                        logit, mask = data_parallel(net, image)\n","                        loss0 = F.cross_entropy(logit, label)\n","                        loss1 = L1_FACTOR*F.binary_cross_entropy_with_logits(mask, truth_mask)\n","\n","                    #scaler.scale(loss0).backward()\n","                    #scaler.scale(loss1).backward()\n","                    scaler.scale(loss0+loss1).backward()\n","                    scaler.unscale_(optimizer)\n","                    #torch.nn.utils.clip_grad_norm_(net.parameters(), 2)\n","                    scaler.step(optimizer)\n","                    scaler.update()\n","\n","\n","                else :\n","                    assert(False)\n","                    print('fp32')\n","                    logit, mask = data_parallel(net, image)\n","                    loss0 = F.cross_entropy(logit, label)\n","                    loss1 = F.binary_cross_entropy_with_logits(mask, truth_mask.shape)\n","\n","                    (loss0+loss1).backward()\n","                    optimizer.step()\n","\n","\n","                # print statistics  --------\n","                epoch += 1 / len(train_loader1)\n","                iteration += 1\n","\n","                batch_loss = np.array([loss0.item(), loss1.item(), loss2.item()])\n","                sum_train_loss += batch_loss\n","                sum_train += 1\n","                if iteration % 100 == 0:\n","                    train_loss = sum_train_loss / (sum_train + 1e-12)\n","                    sum_train_loss[...] = 0\n","                    sum_train = 0\n","\n","                print('\\r', end='', flush=True)\n","                print(message(mode='print'), end='', flush=True)\n","\n","\n","        log.write('\\n')"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"3_7GTDVDz3Io","executionInfo":{"status":"error","timestamp":1625787875515,"user_tz":-180,"elapsed":5880326,"user":{"displayName":"Drew Wert","photoUrl":"","userId":"15737305534098303053"}},"outputId":"85f18fc5-1252-43ea-88b5-5c6fe39390c2"},"source":["run_train_mixup_cutmix()"],"execution_count":28,"outputs":[{"output_type":"stream","text":["\n","--- [START 2021-07-08_13-11-27] ----------------------------------------------------------------\n","\n","\t@common.py:  \n","\tpytorch\n","\t\tseed = 1625749894\n","\t\ttorch.__version__              = 1.9.0+cu102\n","\t\ttorch.version.cuda             = 10.2\n","\t\ttorch.backends.cudnn.version() = 7605\n","\t\tos['CUDA_VISIBLE_DEVICES']     = 0\n","\t\ttorch.cuda.device_count()      = 1\n","\t\ttorch.cuda.get_device_properties() = (name='Tesla P100-PCIE-16GB', major=6, minor=0, total_memory=16280MB, multi_processor_count=56)\n","\n","\n","\texpirement = SZ640_D201_MASK_0FOLD_WL4_MIXUP_CUTMIX\n","\tout_dir  = /content/drive/My Drive/kaggle/covid19-det/output/SZ640_D201_MASK_0FOLD_WL4_MIXUP_CUTMIX/fold0-fine5\n","\n","train_dataset : \n","\tlen = 4980\n","\tdf  = (4980, 10)\n","\tlabel distribution\n","\t\t 0     Negative for Pneumonia:  1366 (0.2743)\n","\t\t 1         Typical Appearance:  2367 (0.4753)\n","\t\t 2   Indeterminate Appearance:   864 (0.1735)\n","\t\t 3        Atypical Appearance:   383 (0.0769)\n","\n","valid_dataset : \n","\tlen = 1247\n","\tdf  = (1247, 10)\n","\tlabel distribution\n","\t\t 0     Negative for Pneumonia:   343 (0.2751)\n","\t\t 1         Typical Appearance:   591 (0.4739)\n","\t\t 2   Indeterminate Appearance:   218 (0.1748)\n","\t\t 3        Atypical Appearance:    95 (0.0762)\n","\n","\n","** net setting **\n","net=<class '__main__.AmpNet'>\n","\tinitial_checkpoint = /content/drive/My Drive/kaggle/covid19-det/output/SZ640_D201_MASK_0FOLD_WL4_MIXUP_CUTMIX/fold0-fine4/checkpoint/best_model.pth\n","\n","optimizer\n","  MADGRAD (\n","Parameter Group 0\n","    eps: 1e-06\n","    lr: 1e-05\n","    momentum: 0.9\n","    weight_decay: 0\n",")\n","\n","** start training here! **\n","   fold = 0\n","   is_mixed_precision = True \n","   batch_size = 8\n","   experiment = ['SZ640_D201_MASK_0FOLD_WL4_MIXUP_CUTMIX']\n","                      |-----------VALID-------------|-----TRAIN/BATCH -----|------TIME-----\n","rate     iter   epoch | loss    map   topk0  topk1  | loss0  loss1  loss2  |               \n","-------------------------------------------------------------------------------------------\n","0.00000  2.420* 38.91  | 0.948  0.363  0.666  0.836  | 0.000  0.000  0.000  |  0 hr 00 min\n","0.00001  2.440  39.23  | 0.980  0.359  0.650  0.826  | 0.951  0.782  0.000  |  0 hr 03 min\n","0.00001  2.460  39.55  | 0.988  0.359  0.650  0.823  | 0.954  0.783  0.000  |  0 hr 05 min\n","0.00001  2.480  39.87  | 0.973  0.360  0.655  0.828  | 0.940  0.794  0.000  |  0 hr 08 min\n","0.00001  2.500  40.19  | 0.984  0.362  0.660  0.835  | 0.890  0.789  0.000  |  0 hr 11 min\n","0.00001  2.520  40.51  | 0.971  0.361  0.658  0.824  | 0.971  0.792  0.000  |  0 hr 13 min\n","0.00001  2.540  40.84  | 0.989  0.360  0.650  0.828  | 0.895  0.773  0.000  |  0 hr 16 min\n","0.00001  2.560  41.16  | 0.967  0.362  0.661  0.840  | 0.898  0.784  0.000  |  0 hr 19 min\n","0.00001  2.580  41.48  | 1.020  0.361  0.665  0.829  | 0.939  0.787  0.000  |  0 hr 22 min\n","0.00001  2.600  41.80  | 0.975  0.360  0.650  0.829  | 0.936  0.761  0.000  |  0 hr 24 min\n","0.00001  2.620  42.12  | 0.968  0.361  0.659  0.832  | 0.902  0.773  0.000  |  0 hr 27 min\n","0.00001  2.640  42.44  | 0.965  0.361  0.658  0.832  | 0.892  0.772  0.000  |  0 hr 30 min\n","0.00001  2.660  42.77  | 0.963  0.360  0.658  0.832  | 0.924  0.782  0.000  |  0 hr 32 min\n","0.00001  2.680  43.09  | 0.979  0.361  0.663  0.842  | 0.874  0.809  0.000  |  0 hr 35 min\n","0.00001  2.700  43.41  | 0.976  0.360  0.657  0.830  | 0.884  0.814  0.000  |  0 hr 38 min\n","0.00001  2.720  43.73  | 0.977  0.360  0.655  0.828  | 0.962  0.769  0.000  |  0 hr 41 min\n","0.00001  2.740  44.05  | 0.956  0.361  0.664  0.840  | 0.929  0.784  0.000  |  0 hr 43 min\n","0.00001  2.760  44.37  | 1.002  0.358  0.649  0.822  | 0.912  0.771  0.000  |  0 hr 46 min\n","0.00001  2.780  44.69  | 1.021  0.359  0.654  0.827  | 0.883  0.763  0.000  |  0 hr 49 min\n","0.00001  2.800  45.02  | 0.994  0.361  0.662  0.835  | 0.914  0.833  0.000  |  0 hr 51 min\n","0.00001  2.820  45.34  | 0.978  0.361  0.657  0.829  | 0.952  0.767  0.000  |  0 hr 54 min\n","0.00001  2.840  45.66  | 1.010  0.360  0.656  0.822  | 0.936  0.772  0.000  |  0 hr 57 min\n","0.00001  2.860  45.98  | 0.987  0.362  0.664  0.833  | 0.870  0.784  0.000  |  1 hr 00 min\n","0.00001  2.880  46.30  | 0.982  0.362  0.652  0.829  | 0.905  0.791  0.000  |  1 hr 02 min\n","0.00001  2.900  46.62  | 0.959  0.362  0.655  0.832  | 0.932  0.783  0.000  |  1 hr 05 min\n","0.00001  2.920  46.95  | 0.962  0.361  0.658  0.832  | 0.955  0.751  0.000  |  1 hr 08 min\n","0.00001  2.940  47.27  | 0.949  0.361  0.655  0.835  | 0.942  0.769  0.000  |  1 hr 10 min\n","0.00001  2.960  47.59  | 0.966  0.362  0.656  0.834  | 0.897  0.800  0.000  |  1 hr 13 min\n","0.00001  2.980  47.91  | 0.982  0.361  0.659  0.829  | 0.938  0.749  0.000  |  1 hr 16 min\n","0.00001  3.000  48.23  | 0.982  0.360  0.656  0.832  | 0.905  0.780  0.000  |  1 hr 18 min\n","0.00001  3.020  48.55  | 0.971  0.360  0.660  0.829  | 0.890  0.787  0.000  |  1 hr 21 min\n","0.00001  3.040  48.87  | 0.957  0.361  0.662  0.832  | 0.898  0.781  0.000  |  1 hr 24 min\n","0.00001  3.060* 49.20  | 0.981  0.363  0.665  0.840  | 0.866  0.832  0.000  |  1 hr 27 min\n","0.00001  3.080  49.52  | 1.020  0.360  0.650  0.825  | 0.888  0.808  0.000  |  1 hr 29 min\n","0.00001  3.100  49.84  | 0.977  0.361  0.655  0.829  | 0.884  0.781  0.000  |  1 hr 32 min\n","0.00001  3.120  50.16  | 0.992  0.358  0.650  0.828  | 0.867  0.768  0.000  |  1 hr 35 min\n","0.00001  3.140  50.48  | 0.981  0.361  0.653  0.830  | 0.927  0.738  0.000  |  1 hr 37 min\n","0.00001  3.142  50.51  | 0.981  0.361  0.653  0.830  | 1.007  0.579  0.000  |  1 hr 37 min"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-0ef16487a0c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_train_mixup_cutmix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-27-f30404e67ec6>\u001b[0m in \u001b[0;36mrun_train_mixup_cutmix\u001b[0;34m()\u001b[0m\n\u001b[1;32m    220\u001b[0m                     \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;31m#torch.nn.utils.clip_grad_norm_(net.parameters(), 2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                     \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m                     \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"No inf checks were recorded for this optimizer.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"Zp1gDsfmQ0eH"},"source":["# start here ! ###################################################################################\n","def run_train():\n","    for fold in FOLDS_SET:\n","        out_dir = OUTPUT_DIR['NORM']+EXPERIMENT+'/fold%d-fine'%fold\n","        \n","        initial_checkpoint=None\n","        if INITIAL_CHECKPOINTS[fold] is not None: \n","          initial_checkpoint = OUTPUT_DIR['NORM']+EXPERIMENT+'/fold%d/checkpoint/'%fold+INITIAL_CHECKPOINTS[fold]\n","\n","        best_map = 0.3\n","        start_lr   = 0.0001#1\n","        batch_size = 8 #14 #22\n","\n","        num_iteration = 8000\n","        iter_log    = 200\n","        iter_valid  = 200\n","        iter_save   = list(range(0, num_iteration+1, 200))\n","        a_iter_save = []\n","\n","        ## setup  ----------------------------------------\n","        for f in ['checkpoint', 'train', 'valid', 'backup']: os.makedirs(out_dir + '/' + f, exist_ok=True)\n","        # backup_project_as_zip(PROJECT_PATH, out_dir +'/backup/code.train.%s.zip'%IDENTIFIER)\n","\n","        log = Logger()\n","        log.open(out_dir + '/log.train.txt', mode='a')\n","        log.write('\\n--- [START %s] %s\\n\\n' % (IDENTIFIER, '-' * 64))\n","        log.write('\\t%s\\n' % COMMON_STRING)\n","        log.write('\\texpirement = %s\\n' % EXPERIMENT)\n","        log.write('\\tout_dir  = %s\\n' % out_dir)\n","        log.write('\\n')\n","\n","        ## dataset ------------------------------------\n","        df_train, df_valid = make_fold('train-%d'%fold)\n","        train_dataset = SiimDataset(df_train, train_augment)\n","        valid_dataset = SiimDataset(df_valid, )\n","\n","        train_loader = DataLoader(\n","            train_dataset,\n","            sampler = RandomSampler(train_dataset),\n","            batch_size = batch_size,\n","            drop_last   = True,\n","            num_workers = 2,\n","            pin_memory  = True,\n","            worker_init_fn=lambda id: np.random.seed(torch.initial_seed() // 2 ** 32 + id),\n","            collate_fn  = null_collate,\n","        )\n","        valid_loader  = DataLoader(\n","            valid_dataset,\n","            sampler = SequentialSampler(valid_dataset),\n","            batch_size  = 16,\n","            drop_last   = False,\n","            num_workers = 2,\n","            pin_memory  = True,\n","            collate_fn  = null_collate,\n","        )\n","\n","        log.write('train_dataset : \\n%s\\n'%(train_dataset))\n","        log.write('valid_dataset : \\n%s\\n'%(valid_dataset))\n","        log.write('\\n')\n","\n","\n","        ## net ----------------------------------------\n","        log.write('** net setting **\\n')\n","        if is_mixed_precision:\n","            scaler = amp.GradScaler()\n","            net = AmpNet().cuda()\n","        else:\n","            net = Net().cuda()\n","\n","\n","        if initial_checkpoint is not None:\n","            f = torch.load(initial_checkpoint, map_location=lambda storage, loc: storage)\n","            start_iteration = f['iteration']\n","            num_iteration = start_iteration + 8000\n","            start_epoch = f['epoch']\n","            state_dict  = f['state_dict']\n","            net.load_state_dict(state_dict,strict=True)  #True\n","        else:\n","            start_iteration = 0\n","            start_epoch = 0\n","\n","\n","        log.write('net=%s\\n'%(type(net)))\n","        log.write('\\tinitial_checkpoint = %s\\n' % initial_checkpoint)\n","        log.write('\\n')\n","\n","        # -----------------------------------------------\n","        if 0: ##freeze\n","            for p in net.block0.backbone.parameters(): p.requires_grad = False\n","\n","\n","        #optimizer = Lookahead(RAdam(filter(lambda p: p.requires_grad, net.parameters()),lr=start_lr), alpha=0.5, k=5)\n","        #optimizer = RAdam(filter(lambda p: p.requires_grad, net.parameters()),lr=start_lr)\n","        optimizer = MADGRAD( filter(lambda p: p.requires_grad, net.parameters()), lr=start_lr, momentum= 0.9, weight_decay= 0, eps= 1e-06)\n","\n","\n","        # num_iteration = 8000\n","        # iter_log    = 100\n","        # iter_valid  = 100\n","        # iter_save   = list(range(0, num_iteration, 100))#1*1000\n","\n","        log.write('optimizer\\n  %s\\n'%(optimizer))\n","        log.write('\\n')\n","\n","\n","        ## start training here! ##############################################\n","        log.write('** start training here! **\\n')\n","        log.write('   fold = %d\\n'%(fold))\n","        log.write('   is_mixed_precision = %s \\n'%str(is_mixed_precision))\n","        log.write('   batch_size = %d\\n'%(batch_size))\n","        log.write('   experiment = %s\\n' % str(EXPERIMENT.split('/')[-2:]))\n","        log.write('                      |-----------VALID-------------|-----TRAIN/BATCH -----|------TIME-----\\n')\n","        log.write('rate     iter   epoch | loss    map   topk0  topk1  | loss0  loss1  loss2  |               \\n')\n","        log.write('-------------------------------------------------------------------------------------------\\n')\n","                  #0.00000  0.00*  0.00  | 0.000  0.000  0.000  0.000  | 0.000  0.000  0.000  |  0 hr 00 min\n","\n","        def message(mode='print'):\n","            if mode==('print'):\n","                asterisk = ' '\n","                loss = batch_loss\n","            if mode==('log'):\n","                asterisk = '*' if iteration in a_iter_save else ' '\n","                loss = train_loss\n","\n","            text = \\\n","                '%0.5f  %5.3f%s %4.2f  | '%(rate, iteration/10000, asterisk, epoch,) +\\\n","                '%4.3f  %4.3f  %4.3f  %4.3f  | '%(*valid_loss,) +\\\n","                '%4.3f  %4.3f  %4.3f  | '%(*loss,) +\\\n","                '%s' % (time_to_str(timer() - start_timer,'min'))\n","\n","            return text\n","\n","        #----\n","        valid_loss = np.zeros(4,np.float32)\n","        train_loss = np.zeros(3,np.float32)\n","        batch_loss = np.zeros_like(train_loss)\n","        sum_train_loss = np.zeros_like(train_loss)\n","        sum_train = 0\n","        loss0 = torch.FloatTensor([0]).cuda().sum()\n","        loss1 = torch.FloatTensor([0]).cuda().sum()\n","        loss2 = torch.FloatTensor([0]).cuda().sum()\n","\n","\n","        start_timer = timer()\n","        iteration = start_iteration\n","        epoch = start_epoch\n","        rate = 0\n","        while  iteration < num_iteration:\n","            for t, batch in enumerate(train_loader):\n","\n","                if (iteration % iter_valid == 0):\n","                    #if iteration!=start_iteration:\n","                        valid_loss = do_valid(net, valid_loader)  #\n","                        pass\n","\n","\n","                if best_map < valid_loss[1]:\n","                  best_map = valid_loss[1]\n","                  a_iter_save.append(iteration)\n","                  torch.save({\n","                            'state_dict': net.state_dict(),\n","                            'iteration': iteration,\n","                            'epoch': epoch,\n","                        }, out_dir + '/checkpoint/best_model.pth')\n","                  pass\n","\n","                if (iteration % iter_log == 0):\n","                    print('\\r', end='', flush=True)\n","                    log.write(message(mode='log') + '\\n')\n","\n","\n","                # learning rate schduler ------------\n","                rate = get_learning_rate(optimizer)\n","\n","                # one iteration update  -------------\n","                batch_size = len(batch['index'])\n","                image = batch['image'].cuda()\n","                truth_mask = batch['mask'].cuda()\n","                truth_mask = F.interpolate(truth_mask, size=MASK_SIZE, mode='bilinear', align_corners=False)\n","                onehot = batch['onehot'].cuda()\n","                label = onehot.argmax(-1)\n","\n","                #----\n","                net.train()\n","                optimizer.zero_grad()\n","\n","                if is_mixed_precision:\n","                    with amp.autocast():\n","                        logit, mask = data_parallel(net, image)\n","                        loss0 = F.cross_entropy(logit, label)\n","                        loss1 = 4*F.binary_cross_entropy_with_logits(mask, truth_mask)\n","\n","                    #scaler.scale(loss0).backward()\n","                    #scaler.scale(loss1).backward()\n","                    scaler.scale(loss0+loss1).backward()\n","                    scaler.unscale_(optimizer)\n","                    #torch.nn.utils.clip_grad_norm_(net.parameters(), 2)\n","                    scaler.step(optimizer)\n","                    scaler.update()\n","\n","\n","                else :\n","                    assert(False)\n","                    print('fp32')\n","                    logit, mask = data_parallel(net, image)\n","                    loss0 = F.cross_entropy(logit, label)\n","                    loss1 = F.binary_cross_entropy_with_logits(mask, truth_mask.shape)\n","\n","                    (loss0+loss1).backward()\n","                    optimizer.step()\n","\n","\n","                # print statistics  --------\n","                epoch += 1 / len(train_loader)\n","                iteration += 1\n","\n","                batch_loss = np.array([loss0.item(), loss1.item(), loss2.item()])\n","                sum_train_loss += batch_loss\n","                sum_train += 1\n","                if iteration % 100 == 0:\n","                    train_loss = sum_train_loss / (sum_train + 1e-12)\n","                    sum_train_loss[...] = 0\n","                    sum_train = 0\n","\n","                print('\\r', end='', flush=True)\n","                print(message(mode='print'), end='', flush=True)\n","\n","\n","        log.write('\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YTvnKVkBSUrp"},"source":["run_train()"],"execution_count":null,"outputs":[]}]}